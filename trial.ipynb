{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import special\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES = 10\n",
    "TAU_LIMIT = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6613\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "class Environment1:\n",
    "    def __init__(self) -> None:\n",
    "        self.k = 1\n",
    "        self.sigma2 = 0.6613\n",
    "        self.zeta = 45\n",
    "        self.L = 1\n",
    "        self.P_bar = .6613\n",
    "        self.h = np.array([self.P_bar])\n",
    "        self.A = np.array([1.2])\n",
    "        self.channel_states = (np.array(range(10)) + 1) * .2\n",
    "        self.eta = np.array([[.91,.09,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "                             [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "                             [ .8,.12,.08,  0,  0,  0,  0,  0,  0,  0],\n",
    "                             [ .7,.11,.11,.08,  0,  0,  0,  0,  0,  0],\n",
    "                             [ .6,.11, .1, .1,.09,  0,  0,  0,  0,  0],\n",
    "                             [ .5,.18, .1,.08,.07,.07,  0,  0,  0,  0],\n",
    "                             [ .4,.28, .1,.06,.05,.06,.05,  0,  0,  0],\n",
    "                             [ .3, .1, .1, .1,.16,.14,.09,.01,  0,  0],\n",
    "                             [ .2,.11, .1, .1, .1,.05,.09,.13,.12,  0],\n",
    "                             [ .1, .1, .1, .1, .1, .1, .1, .1, .1,  0]])\n",
    "        \n",
    "    \n",
    "    def robot_reward(self, state, action):\n",
    "        '''Gives the one-step reward for a given state-action pair'''\n",
    "        tau = int(state / N_STATES)\n",
    "\n",
    "        # If tau is 0, error is just the single transmission error\n",
    "        if tau == 0:\n",
    "            self.h = self.P_bar\n",
    "        else:\n",
    "            self.h = self.A**2 * self.h\n",
    "            \n",
    "        # The following just has to do with ugly implementation\n",
    "        if type(self.h) == float:\n",
    "            return self.h + action * self.k * self.zeta\n",
    "        if  self.h.size == 1:\n",
    "            return self.h[0] + action * self.k * self.zeta\n",
    "        return np.trace(self.h) + action * self.k * self.zeta\n",
    "    \n",
    "    def f(self, SNR):\n",
    "        '''This is an internal function to do with the probability of recieving properly (SNR stuff)'''\n",
    "        two_F = special.erf(SNR) / np.sqrt(2)\n",
    "        return (1 - two_F) ** self.L\n",
    "    \n",
    "    def new_state(self, state, action):\n",
    "        '''This generates a new state given the old state-action pair'''\n",
    "        # States are saved as: (tau * N_STATES) + h\n",
    "\n",
    "        # Unwrapping state into tau and h\n",
    "        tau = int(state / N_STATES)\n",
    "        h = state % N_STATES\n",
    "\n",
    "        # Generating a new h based on eta\n",
    "        new_h = random.choices(range(N_STATES), weights=self.eta[h,:N_STATES], k=1)[0]\n",
    "\n",
    "        # Assuming a failure of reception\n",
    "        new_tau = tau + 1\n",
    "\n",
    "        if action != 0:\n",
    "            # Reception can only happen if transmission, depending on probability p_j that depends on the SNR\n",
    "            SNR = self.channel_states[h] * self.zeta / self.sigma2\n",
    "            p_j = self.f(SNR)\n",
    "\n",
    "            # Making tau 0 with that probability\n",
    "            r = random.random()\n",
    "            if r > p_j:\n",
    "                new_tau = 0\n",
    "        \n",
    "        # Limitting tau to the TAU_LIMIT\n",
    "        new_tau = new_tau % (TAU_LIMIT+1)\n",
    "\n",
    "        # Wrapping the state (new_h, new_tau) and returning an int\n",
    "        return int((new_tau * N_STATES) + new_h)\n",
    "\n",
    "    def generate_episode(self, policy, length_of_episode, starting_state=random.randint(0,N_STATES)):\n",
    "        episode = []\n",
    "        curr_state = starting_state\n",
    "        for k in range(length_of_episode):\n",
    "            a = policy(curr_state)\n",
    "            r = self.robot_reward(curr_state, a)\n",
    "            episode.append((curr_state, a, r))\n",
    "            curr_state = self.new_state(curr_state, a)\n",
    "        return episode\n",
    "    \n",
    "env = Environment1()\n",
    "print(env.robot_reward(0,0))\n",
    "\n",
    "print(env.new_state(0, 0))\n",
    "print(env.new_state(0, 0))\n",
    "print(env.new_state(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 100001\n",
      "[[1 1 1 1 0 1 1 1 0 1]\n",
      " [1 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 0 1 0 1 0]\n",
      " [0 1 0 1 1 1 0 0 0 1]\n",
      " [1 0 1 1 1 1 1 1 1 0]\n",
      " [1 0 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 1 1 1 1 1 1]\n",
      " [1 0 1 1 1 1 1 1 1 1]\n",
      " [0 0 0 1 1 1 1 1 1 1]\n",
      " [0 0 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "class Greedy:\n",
    "    '''This Greedy is not altruistic at all, i.e., alpha = 0'''\n",
    "    def __init__(self, n_states, n_actions, env) -> None:\n",
    "        self.values = np.abs(np.random.randn(n_states, n_actions))\n",
    "        self.counts = np.zeros((n_states, n_actions))\n",
    "\n",
    "        self.env = env\n",
    "    \n",
    "    def action_to_take(self, state):\n",
    "        '''Decides the chosen arm, based on the estimate values(saved)'''\n",
    "\n",
    "        # Unwrapping the state\n",
    "        tau = int(state/N_STATES)\n",
    "        h = state % N_STATES\n",
    "\n",
    "        # If tau is too high, and channel gain allows good SNR, we transmit\n",
    "        if tau >= 5 and h > 3:\n",
    "            return 1\n",
    "        \n",
    "        # If we don't know what's higher value based on estimated values, random choice\n",
    "        if self.values[state, 0] == self.values[state, 1]:\n",
    "            r = random.random()\n",
    "            if r > .5:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "        # Else, return the best\n",
    "        return np.argmax(self.values[state, :])\n",
    "    \n",
    "    def train_single_state(self, state):\n",
    "        '''Trains the robot to update its estimates'''\n",
    "        # Estimates the action to be taken\n",
    "        a = self.action_to_take(state)\n",
    "\n",
    "        # Checks the reward\n",
    "        r = self.env.robot_reward(state, a)\n",
    "\n",
    "        # Updates the reward as the average of observed rewards\n",
    "        temp = self.values[state, a] * self.counts[state, a] + r\n",
    "        self.counts[state, a] += 1\n",
    "        self.values[state, a] = temp / self.counts[state, a]\n",
    "\n",
    "# Create an object of this policy\n",
    "p1 = Greedy(N_STATES*(TAU_LIMIT+1), 2, env)\n",
    "\n",
    "# Training it\n",
    "s = 2\n",
    "count = 0\n",
    "while 1:\n",
    "    old = np.copy(p1.values)\n",
    "    p1.train_single_state(s)\n",
    "\n",
    "    s = env.new_state(s, p1.action_to_take(s))\n",
    "    n = np.copy(p1.values)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    if np.linalg.norm(old-n) < 1e-9 and count > 100000:\n",
    "        print(\"Count:\", count)\n",
    "        break\n",
    "\n",
    "policy = np.array([[p1.action_to_take(N_STATES*i + j) for j in range(N_STATES)] for i in range(TAU_LIMIT+1) ])\n",
    "print(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([608., 167.,  63.,  63.,  59.,  26.,  10.,   2.,   1.,   1.]),\n",
       " array([ 0.,  9., 18., 27., 36., 45., 54., 63., 72., 81., 90.]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Environment2:\n",
    "    ''''This was developed for Part B of the paper's simulations section'''\n",
    "    def __init__(self) -> None:\n",
    "        self.k = 1\n",
    "        self.sigma2 = 0.6613\n",
    "        self.zeta = 45\n",
    "        self.L = 1\n",
    "        self.P_bar = .6613\n",
    "        self.h = np.array([self.P_bar])\n",
    "        self.A = np.array([1.2])\n",
    "        self.channel_states = (np.array(range(10)) + 1) * .2\n",
    "        self.eta = np.array([[.91,.09,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "                             [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "                             [ .8,.12,.08,  0,  0,  0,  0,  0,  0,  0],\n",
    "                             [ .7,.11,.11,.08,  0,  0,  0,  0,  0,  0],\n",
    "                             [ .6,.11, .1, .1,.09,  0,  0,  0,  0,  0],\n",
    "                             [ .5,.18, .1,.08,.07,.07,  0,  0,  0,  0],\n",
    "                             [ .4,.28, .1,.06,.05,.06,.05,  0,  0,  0],\n",
    "                             [ .3, .1, .1, .1,.16,.14,.09,.01,  0,  0],\n",
    "                             [ .2,.11, .1, .1, .1,.05,.09,.13,.12,  0],\n",
    "                             [ .1, .1, .1, .1, .1, .1, .1, .1, .1,  0]])\n",
    "        \n",
    "    \n",
    "    def robot_reward(self, state, action):\n",
    "        tau = int(state / N_STATES)\n",
    "\n",
    "        # If tau is 0, error is just the single transmission error\n",
    "        if tau == 0:\n",
    "            self.h = self.P_bar\n",
    "        else:\n",
    "            self.h = self.A**2 * self.h\n",
    "\n",
    "        # The following just has to do with ugly implementation\n",
    "        if type(self.h) == float:\n",
    "            return self.h + action * self.k * self.zeta\n",
    "        if  self.h.size == 1:\n",
    "            return self.h[0] + action * self.k * self.zeta\n",
    "        return np.trace(self.h) + action * self.k * self.zeta\n",
    "    \n",
    "    def f(self, SNR):\n",
    "        '''This is an internal function to do with the probability of recieving properly (SNR stuff)'''\n",
    "        two_F = special.erf(SNR) / np.sqrt(2)\n",
    "        return (1 - two_F) ** self.L\n",
    "    \n",
    "    def new_state(self, state, action):\n",
    "        '''This generates a new state given the old state-action pair'''\n",
    "        tau = int(state / N_STATES)\n",
    "        h = state % N_STATES\n",
    "\n",
    "        new_h = random.choices(range(N_STATES), weights=self.eta[h,:N_STATES], k=1)[0]\n",
    "\n",
    "        new_tau = tau + 1\n",
    "\n",
    "        if action != 0:\n",
    "            SNR = self.channel_states[h] * self.zeta / self.sigma2\n",
    "            p_j = self.f(SNR)\n",
    "            r = random.random()\n",
    "            if r > p_j:\n",
    "                new_tau = 0\n",
    "        \n",
    "        new_tau = new_tau % (TAU_LIMIT+1)\n",
    "        return int((new_tau * N_STATES) + new_h)\n",
    "    \n",
    "    def generate_episode(self, policy, length_of_episode, starting_state=random.randint(0,N_STATES)):\n",
    "        '''To generate an episode of length 'length_of_episode', using policy 'policy' '''\n",
    "        episode = []\n",
    "        curr_state = starting_state\n",
    "        for k in range(length_of_episode):\n",
    "            a = policy(curr_state)\n",
    "            r = self.robot_reward(curr_state, a)\n",
    "            episode.append((curr_state, a, r))\n",
    "            curr_state = self.new_state(curr_state, a)\n",
    "        return episode\n",
    "\n",
    "\n",
    "env2 = Environment1()\n",
    "\n",
    "# To test the functions\n",
    "# print(env2.robot_reward(0,0))\n",
    "# print(env2.new_state(0, 0))\n",
    "\n",
    "# Generate an episode to understand probabilities, etc.\n",
    "episode = (env2.generate_episode(p1.action_to_take, 1000))\n",
    "\n",
    "# Unwrapping to just the states\n",
    "states = []\n",
    "for i in range(len(episode)):\n",
    "    s, _, _ = episode[i]\n",
    "    states.append(s)\n",
    "\n",
    "# Drawing a histogram of the states, to understand which states are transitioned to more often\n",
    "plt.hist(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained one step!\n",
      "[[1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 0 1 1 0 0 1 1 1]\n",
      " [1 1 1 1 0 0 1 0 0 0]\n",
      " [0 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 0 0 0 1 1 1 1 1]\n",
      " [0 0 1 0 0 1 1 1 1 1]\n",
      " [1 0 0 0 0 1 1 1 1 1]\n",
      " [1 1 0 0 1 1 1 1 1 1]\n",
      " [0 0 0 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "class Greedy_per_episode:\n",
    "    def __init__(self, n_states, n_actions, env) -> None:\n",
    "        self.values = np.zeros((n_states, n_actions))\n",
    "        self.counts = np.zeros((n_states, n_actions))\n",
    "        self.alpha = 0.6\n",
    "\n",
    "        self.env = env\n",
    "    \n",
    "    def action_to_take(self, state):\n",
    "        tau = int(state/N_STATES)\n",
    "        h = state % N_STATES\n",
    "        if tau >= 5 and h > 3:\n",
    "            return 1\n",
    "        \n",
    "        if self.values[state, 0] == self.values[state, 1]:\n",
    "            r = random.random()\n",
    "            if r > .5:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        return np.argmax(self.values[state, :])\n",
    "    \n",
    "    def policy(self):\n",
    "        return np.array([[self.action_to_take(N_STATES*i + j) for i in range(TAU_LIMIT+1)] for j in range(N_STATES)])\n",
    "    \n",
    "    def train_episode(self, episode):\n",
    "        curr_state_idx = len(episode) - 1\n",
    "        curr_val = 0\n",
    "        \n",
    "        while curr_state_idx >= 0:\n",
    "            s , a, r = episode[curr_state_idx]\n",
    "            curr_state_idx -= 1\n",
    "            tau = int(s/N_STATES)\n",
    "            h = s % N_STATES\n",
    "            if tau >= 5 and h > 3:\n",
    "                continue\n",
    "            curr_val = (curr_val*self.alpha) + r\n",
    "            temp = self.values[s,a] * self.counts[s,a]\n",
    "            self.counts[s, a] += 1\n",
    "\n",
    "            self.values[s,a] = (temp + curr_val) / self.counts[s,a]\n",
    "\n",
    "\n",
    "p2 = Greedy_per_episode(n_states=N_STATES*(TAU_LIMIT+1), n_actions=2, env=env2)\n",
    "\n",
    "ep = env2.generate_episode(p2.action_to_take, 100000)\n",
    "p2.train_episode(ep)\n",
    "\n",
    "print(\"Trained one step!\")\n",
    "\n",
    "print(p2.policy())\n",
    "\n",
    "# print(np.reshape(np.floor(p2.values), (2, 10,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 0 1 0 1 0 0 0]\n",
      " [0 0 1 0 0 1 1 1 0 1]\n",
      " [0 1 1 0 1 1 1 1 1 1]\n",
      " [0 1 0 1 1 1 1 1 1 1]\n",
      " [1 0 1 0 1 1 1 1 1 1]\n",
      " [0 0 1 1 1 1 1 1 1 1]\n",
      " [0 1 1 0 0 1 1 1 1 1]\n",
      " [0 1 1 1 0 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    old_values = np.copy(p2.values)\n",
    "    ep = env2.generate_episode(p2.action_to_take, 10000)\n",
    "    p2.train_episode(ep)\n",
    "    new_values = np.copy(p2.values)\n",
    "    if np.linalg.norm(old_values - new_values) < 1e-4:\n",
    "        print(i)\n",
    "        break\n",
    "print(p2.policy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 1 1 1 1 1]\n",
      " [0 0 0 1 1 1 1 1 1 1]\n",
      " [0 1 1 1 0 1 1 1 1 1]\n",
      " [0 1 1 1 1 1 1 1 1 1]\n",
      " [0 1 1 0 0 1 1 1 1 1]\n",
      " [0 1 0 0 1 1 1 1 1 1]\n",
      " [1 0 1 1 1 1 1 1 1 1]\n",
      " [0 0 1 1 0 1 1 1 1 1]\n",
      " [1 1 0 0 0 1 1 1 1 1]\n",
      " [1 0 1 1 0 1 1 1 1 1]]\n",
      "Trained a single step!\n",
      "[[1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 1 1 1 1 1 1 1]\n",
      " [1 1 0 0 0 1 1 1 1 1]\n",
      " [0 0 1 0 1 1 1 1 1 1]\n",
      " [0 0 0 1 0 1 1 1 1 1]\n",
      " [0 1 1 0 1 1 1 1 1 1]\n",
      " [0 1 0 0 0 1 1 1 1 1]\n",
      " [0 1 0 0 1 1 1 1 1 1]\n",
      " [1 1 0 1 0 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "class Greedy_per_episode_v2:\n",
    "    '''This is my attempt at P2_greedy'''\n",
    "    def __init__(self, n_states, n_actions, env) -> None:\n",
    "        self.values = np.zeros((n_states, n_actions))\n",
    "        self.counts = np.zeros((n_states, n_actions))\n",
    "        self.alpha = 0.6\n",
    "\n",
    "        self.env = env\n",
    "    \n",
    "    def action_to_take(self, state):\n",
    "        '''Defines the policy'''\n",
    "\n",
    "        # Unwrapping the state\n",
    "        tau = int(state/N_STATES)\n",
    "        h = state % N_STATES\n",
    "\n",
    "        # Some threshold I don't understand yet\n",
    "        if tau >= 5:\n",
    "            return 1\n",
    "        \n",
    "        # If we have no \"best option\", choose random with probability 0.5\n",
    "        if self.values[state, 0] == self.values[state, 1]:\n",
    "            r = random.random()\n",
    "            if r > .5:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "        # Otherwise, all good. Return the max\n",
    "        return np.argmax(self.values[state, :])\n",
    "    \n",
    "    def policy(self):\n",
    "        '''To get a printable form of the policy (Refer Fig. 2 of Original Paper.pdf)'''\n",
    "        return np.array([[self.action_to_take(N_STATES*i + j) for i in range(TAU_LIMIT+1)] for j in range(N_STATES)])\n",
    "        \n",
    "    def train_episode(self, episode):\n",
    "        '''This trains on episodes as below, Every Visit update, and can be used for ON or OFF policy'''\n",
    "        curr_state_idx = len(episode) - 1\n",
    "        curr_val = 0\n",
    "        \n",
    "        # Starting from the back so we don't have to loop through the rest of the episode for each state\n",
    "        while curr_state_idx >= 0:\n",
    "            # Unwrapping the episode into state, action, reward\n",
    "            s, a, r = episode[curr_state_idx]\n",
    "            # Decrementing the index of the episode step\n",
    "            curr_state_idx -= 1\n",
    "\n",
    "            # Unwrapping the state to get tau alone\n",
    "            tau = int(s/N_STATES)\n",
    "\n",
    "            # The weird, ununderstood threshold\n",
    "            if tau >= 5:\n",
    "                continue\n",
    "\n",
    "            # Update the current value\n",
    "            curr_val = (curr_val*self.alpha) + r\n",
    "\n",
    "            # Update the policy's estimate\n",
    "            temp = self.values[s,a] * self.counts[s,a]\n",
    "            self.counts[s, a] += 1\n",
    "            self.values[s,a] = (temp + curr_val) / self.counts[s,a]\n",
    "\n",
    "\n",
    "p3 = Greedy_per_episode_v2(n_states=100, n_actions=2, env=env2)\n",
    "\n",
    "# print(np.reshape((p2.values), (2, 10,10)))\n",
    "print(p3.policy())\n",
    "\n",
    "ep = env2.generate_episode(p2.action_to_take, 100000)\n",
    "p3.train_episode(ep)\n",
    "\n",
    "print(\"Trained a single step!\")\n",
    "\n",
    "# print(p1.values)\n",
    "print(p3.policy())\n",
    "\n",
    "# print(np.reshape(np.floor(p2.values), (2, 10,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]\n",
      " [0 0 0 0 0 1 1 1 1 1]\n",
      " [0 1 0 1 0 1 1 1 1 1]\n",
      " [1 0 0 1 0 1 1 1 1 1]\n",
      " [1 1 1 0 0 1 1 1 1 1]\n",
      " [1 1 0 1 0 1 1 1 1 1]\n",
      " [0 0 1 0 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Training over multiple episodes\n",
    "for i in range(500):\n",
    "    # For convergence testing, save the original values\n",
    "    old_values = np.copy(p3.values)\n",
    "\n",
    "    # Generate an episode\n",
    "    ep = env2.generate_episode(p3.action_to_take, 10000)\n",
    "\n",
    "    # Train based on the generated episode\n",
    "    p3.train_episode(ep)\n",
    "\n",
    "    # For convergence testing, save the new values\n",
    "    new_values = np.copy(p3.values)\n",
    "\n",
    "    # If the values have converged, probably no point in training more\n",
    "    if np.linalg.norm(old_values - new_values) < 1e-4:\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "# Printing the trained policy\n",
    "print(p3.policy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate an episode to understand probabilities, etc.\n",
    "episode = (env2.generate_episode(p3.action_to_take, 1000))\n",
    "\n",
    "# Unwrapping to just the states\n",
    "states = []\n",
    "for i in range(len(episode)):\n",
    "    s, _, _ = episode[i]\n",
    "    states.append(s)\n",
    "\n",
    "# Drawing a histogram of the states, to understand which states are transitioned to more often\n",
    "plt.hist(states)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
