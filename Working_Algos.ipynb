{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CHANNEL_STATES = 10\n",
    "TAU_LIMIT = 9\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "TRANSMITED_POWER = 50\n",
    "WEIGHING_FACTOR = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self) -> None:\n",
    "        self.time = 1\n",
    "        self.A = 1.2\n",
    "        self.Q = 1\n",
    "        self.C = 1\n",
    "        self.R = 1\n",
    "        self.sigma_2 = 8 # Noise power\n",
    "        self.packet_length = 10 \n",
    "        self.P_bar = 0.6613\n",
    "        self.zeta = TRANSMITED_POWER # Transmitted energy\n",
    "\n",
    "        self.state = random.randint(0, N_CHANNEL_STATES*(TAU_LIMIT + 1))\n",
    "\n",
    "        self.channel_gains = np.array(range(10)) * 0.2 + 0.2\n",
    "        self.eta = np.array([[.91,.09,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "                             [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "                             [ .8,.12,.08,  0,  0,  0,  0,  0,  0,  0],\n",
    "                             [ .7,.11,.11,.08,  0,  0,  0,  0,  0,  0],\n",
    "                             [ .6,.11, .1, .1,.09,  0,  0,  0,  0,  0],\n",
    "                             [ .5,.18, .1,.08,.07,.07,  0,  0,  0,  0],\n",
    "                             [ .4,.28, .1,.06,.05,.06,.05,  0,  0,  0],\n",
    "                             [ .3, .1, .1, .1,.16,.14,.09,.01,  0,  0],\n",
    "                             [ .2,.11, .1, .1, .1,.05,.09,.13,.12,  0],\n",
    "                             [ .1, .1, .1, .1, .1, .1, .1, .1, .1, .1]])\n",
    "        self.h_tau_P = np.zeros(10)\n",
    "        self.h_tau_P[0] = self.P_bar\n",
    "        for i in range(10):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            self.h_tau_P[i] = self.A ** 2 * self.h_tau_P[i-1]\n",
    "        print(self.h_tau_P)\n",
    "        print(self.channel_gains)\n",
    "\n",
    "        self.p_js = np.zeros(N_CHANNEL_STATES)\n",
    "        for j in range(N_CHANNEL_STATES):\n",
    "            SNR = self.channel_gains[j] * self.zeta / self.sigma_2\n",
    "            self.p_js[j] = self.f(SNR)\n",
    " \n",
    "    def robot_step(self, action):\n",
    "        tau = self.state // N_CHANNEL_STATES\n",
    "        h = self.state % N_CHANNEL_STATES\n",
    "\n",
    "        self.time += 1\n",
    "        self.next_state(action)\n",
    "\n",
    "        return self.h_tau_P[tau] + action * WEIGHING_FACTOR * self.zeta\n",
    "\n",
    "    def f(self, SNR):   # Calculates the probability of successful transmission based on SNR\n",
    "        sqrt_SNR = np.sqrt(SNR * 2)\n",
    "        I = scipy.special.erf(sqrt_SNR)\n",
    "        \n",
    "        return I**self.packet_length\n",
    "\n",
    "    def next_state(self, action):\n",
    "        tau = self.state // N_CHANNEL_STATES\n",
    "        h = self.state % N_CHANNEL_STATES\n",
    "\n",
    "        new_h = random.choices(population=range(N_CHANNEL_STATES), weights=self.eta[h, :], k=1)[0]\n",
    "        new_tau = tau + 1\n",
    "        if action == 1:\n",
    "            r = random.uniform(0,1)\n",
    "            if r < self.p_js[h]:\n",
    "                new_tau = 0\n",
    "        \n",
    "        if new_tau > TAU_LIMIT: # Ceiling functionality\n",
    "            new_tau = TAU_LIMIT\n",
    "        \n",
    "        self.state = (new_tau * N_CHANNEL_STATES) + new_h\n",
    "        \n",
    "    def reset(self):\n",
    "        self.time = 1\n",
    "        self.state = random.randint(0, N_CHANNEL_STATES - 1)\n",
    "\n",
    "    def reset_to_explore_starts(self):\n",
    "        self.time = 1\n",
    "        self.state = random.randint(0, N_CHANNEL_STATES * (TAU_LIMIT + 1) - 1)\n",
    "\n",
    "    def evaluate_policy(self, policy):\n",
    "        runs = 100\n",
    "        length_of_episode = 200\n",
    "        discount = 1\n",
    "        reward = 0\n",
    "        for run in range(runs):\n",
    "            self.reset()\n",
    "            discount = 1\n",
    "            for t in range(length_of_episode):\n",
    "                reward += self.robot_step(policy[self.state]) * discount\n",
    "                discount *= DISCOUNT_FACTOR\n",
    "                # print(reward, discount)\n",
    "        return reward / (runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_Policy:\n",
    "    def __init__(self) -> None:\n",
    "        self.policy = np.array(np.random.rand(N_CHANNEL_STATES * (TAU_LIMIT + 1)) < 0.5, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Greedy_1:\n",
    "    def __init__(self) -> None:\n",
    "        self.values = np.zeros(shape=(N_CHANNEL_STATES * (TAU_LIMIT + 1), 2))\n",
    "        self.counts = np.zeros(shape=(N_CHANNEL_STATES * (TAU_LIMIT + 1), 2))\n",
    "        self.policy = np.array(np.random.rand(N_CHANNEL_STATES * (TAU_LIMIT + 1)) > 0.5, dtype=np.int32)\n",
    "        for tau in range(5, TAU_LIMIT+1):\n",
    "            self.policy[tau*N_CHANNEL_STATES + 5 : (tau+1)*N_CHANNEL_STATES] = 1\n",
    "        self.alpha = 0.8\n",
    "    \n",
    "    def train(self, env):\n",
    "        state = env.state\n",
    "        h = state % N_CHANNEL_STATES\n",
    "        tau = state // N_CHANNEL_STATES\n",
    "\n",
    "\n",
    "        action = self.policy[state]\n",
    "        reward = env.robot_step(action)\n",
    "\n",
    "        if h >= 5 and tau >= 5:\n",
    "            return\n",
    "        \n",
    "        new_state = env.state\n",
    "\n",
    "        self.values[state, action] = ((1 - self.alpha) * self.values[state, action]) + (self.alpha * (reward + DISCOUNT_FACTOR * np.max(self.values[new_state, :])))\n",
    "        self.policy[state] = np.argmin(self.values[state, :])\n",
    "    \n",
    "    def print_policy(self):\n",
    "        print(np.reshape(self.policy, newshape=(N_CHANNEL_STATES, (TAU_LIMIT + 1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Greedy_2:\n",
    "    def __init__(self) -> None:\n",
    "        self.values = np.zeros(shape=(N_CHANNEL_STATES * (TAU_LIMIT + 1), 2))\n",
    "        self.counts = np.zeros(shape=(N_CHANNEL_STATES * (TAU_LIMIT + 1), 2))\n",
    "        self.policy = np.array(np.random.rand(N_CHANNEL_STATES * (TAU_LIMIT + 1)) > 0.5, dtype=np.int32)\n",
    "        self.policy[50:] = 1\n",
    "        self.alpha = 0.8\n",
    "    \n",
    "    def train(self, env):\n",
    "        state = env.state\n",
    "        h = state % N_CHANNEL_STATES\n",
    "        tau = state // N_CHANNEL_STATES\n",
    "\n",
    "\n",
    "        action = self.policy[state]\n",
    "        reward = env.robot_step(action)\n",
    "\n",
    "        if tau >= 5:\n",
    "            return\n",
    "        \n",
    "        new_state = env.state\n",
    "\n",
    "        self.values[state, action] = ((1 - self.alpha) * self.values[state, action]) + (self.alpha * (reward + DISCOUNT_FACTOR * np.max(self.values[new_state, :])))\n",
    "        self.policy[state] = np.argmin(self.values[state, :])\n",
    "    \n",
    "    def print_policy(self):\n",
    "        print(np.reshape(self.policy, newshape=(N_CHANNEL_STATES, (TAU_LIMIT + 1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self) -> None:\n",
    "        self.policy = np.array(np.random.rand(N_CHANNEL_STATES * (TAU_LIMIT + 1)) > 0.5, dtype=np.int32)\n",
    "            # This is initially a random policy, which will be finalized after values are calculated\n",
    "        self.estimated_costs = np.ones(shape=(N_CHANNEL_STATES * (TAU_LIMIT + 1), 2)) * 1000 \n",
    "            # This is to understand the estimated cost of taking an action in a given state, \n",
    "            # which is initialized to a very high value to prevent usage of unexplored states\n",
    "        self.count_of_transitions = np.zeros(shape=(2, N_CHANNEL_STATES * (TAU_LIMIT + 1), N_CHANNEL_STATES * (TAU_LIMIT + 1)))\n",
    "            # This is the count of transitions from state s to state s' given action a\n",
    "        self.estimate_transitions = np.zeros(shape=(2, N_CHANNEL_STATES * (TAU_LIMIT + 1), N_CHANNEL_STATES * (TAU_LIMIT + 1)))\n",
    "            # This is the normalized probability of the transition s -> s' given action a\n",
    "        self.estimate_values = np.zeros(N_CHANNEL_STATES * (TAU_LIMIT + 1))\n",
    "            # Once we calculate the transitions, this is our current estimate of the value of each state\n",
    "        self.n_states = N_CHANNEL_STATES * (TAU_LIMIT + 1)\n",
    "        \n",
    "        self.new_transitions = np.zeros(shape=(2, self.n_states, self.n_states))\n",
    "    \n",
    "    def calculate_transitions(self, env):\n",
    "        self.calculate_transitions_2(env)\n",
    "        self.count_of_transitions += self.estimate_transitions * 10000\n",
    "        env.reset()\n",
    "        for run in range(20000):\n",
    "            # Exploring starts with any state starting\n",
    "            env.reset()\n",
    "            env.state = run % self.n_states\n",
    "            # To generate episodes\n",
    "            for t in range(1000):\n",
    "                # Interact with the environment\n",
    "                s = env.state\n",
    "                a = 1 if run > 10000 else 0\n",
    "                r = env.robot_step(action=a)\n",
    "                s_dash = env.state\n",
    "\n",
    "                # If this is the first exploration of the state, update the estimated cost :\n",
    "                if self.count_of_transitions[a, s, s_dash] == 0:\n",
    "                    self.estimated_costs[s, a] = r\n",
    "\n",
    "                # Update counts:\n",
    "                self.count_of_transitions[a, s, s_dash] += 1\n",
    "        \n",
    "        # Normalize the counts to get transition probabilities:\n",
    "        for s in range(N_CHANNEL_STATES * (TAU_LIMIT + 1)):\n",
    "            for a in range(2):\n",
    "                if np.sum(self.count_of_transitions[a, s, :]) != 0:\n",
    "                    self.estimate_transitions[a, s, :] = self.count_of_transitions[a, s, :] / np.sum(self.count_of_transitions[a, s, :])\n",
    "        print(self.estimate_transitions.shape)\n",
    "\n",
    "    def calculate_transitions_2(self, env):\n",
    "        self.new_transitions = np.zeros(shape=(2, N_CHANNEL_STATES * (TAU_LIMIT + 1), N_CHANNEL_STATES * (TAU_LIMIT + 1)))\n",
    "        for s in range(self.n_states - N_CHANNEL_STATES):\n",
    "            tau = s // N_CHANNEL_STATES\n",
    "            h = s % N_CHANNEL_STATES\n",
    "\n",
    "            new_tau = (tau + 1)\n",
    "            for new_h in range(10):\n",
    "                new_state_0 = (new_tau * N_CHANNEL_STATES) + new_h\n",
    "                new_state_1 = (tau * N_CHANNEL_STATES) + new_h\n",
    "                if new_state_0 < self.n_states:\n",
    "                    self.new_transitions[0, s, new_state_0] = env.eta[h, new_h]\n",
    "                    self.new_transitions[1, s, new_state_0] = env.eta[h, new_h] * (1 - env.p_js[h])\n",
    "                self.new_transitions[1, s, new_state_1] = env.eta[h, new_h] * env.p_js[h]\n",
    "            c = env.h_tau_P[tau]\n",
    "            self.estimated_costs[s, 0] = c\n",
    "            self.estimated_costs[s, 1] = c + env.zeta\n",
    "        \n",
    "        \n",
    "        for s in range(self.n_states - N_CHANNEL_STATES, self.n_states):\n",
    "            tau = s // N_CHANNEL_STATES\n",
    "            h = s % N_CHANNEL_STATES\n",
    "            for new_h in range(10):\n",
    "                new_state = (tau * N_CHANNEL_STATES) + new_h\n",
    "                self.new_transitions[1, s, new_state] = env.eta[h, new_h]\n",
    "                self.new_transitions[0, s, new_state] = env.eta[h, new_h]\n",
    "                # if s == 90:\n",
    "                #     print(s, new_state, env.eta[h, new_h])\n",
    "            c = env.h_tau_P[tau]\n",
    "            self.estimated_costs[s, 1] = c + env.zeta\n",
    "            self.estimated_costs[s, 0] = c\n",
    "\n",
    "    def value_iter_2(self):\n",
    "        count = 0\n",
    "        for i in range(1000):\n",
    "            count += 1\n",
    "            opt1 = self.estimated_costs[:, 0] + DISCOUNT_FACTOR * np.reshape(self.new_transitions[0, :, :], newshape=(self.n_states, self.n_states)) @ self.estimate_values\n",
    "                # Estimated value of choosing to not transmit (action = 0)\n",
    "            opt2 = self.estimated_costs[:, 1] + DISCOUNT_FACTOR * np.reshape(self.new_transitions[1, :, :], newshape=(self.n_states, self.n_states)) @ self.estimate_values\n",
    "                # Estimated value of choosing to transmit (action = 1)\n",
    "            \n",
    "\n",
    "            new_vals = np.min(np.array([opt1, opt2]), axis=0)\n",
    "            # print(new_vals)\n",
    "\n",
    "            if np.linalg.norm(new_vals - self.estimate_values) < 1e-10:\n",
    "                \n",
    "                self.policy = np.argmin(np.array([opt1, opt2]), axis=0)\n",
    "                print(count)\n",
    "                break\n",
    "            self.estimate_values = new_vals\n",
    "\n",
    "        print(count)\n",
    "\n",
    "    def value_iter(self):\n",
    "        count = 0\n",
    "        # print(self.estimated_costs)\n",
    "        for i in range(100):\n",
    "            count += 1\n",
    "            opt1 = self.estimated_costs[:, 0] + DISCOUNT_FACTOR * np.squeeze(self.estimate_transitions[0, :, :]) @ self.estimate_values\n",
    "                # Estimated value of choosing to not transmit (action = 0)\n",
    "            opt2 = self.estimated_costs[:, 1] + DISCOUNT_FACTOR * np.squeeze(self.estimate_transitions[1, :, :]) @ self.estimate_values\n",
    "                # Estimated value of choosing to transmit (action = 1)\n",
    "            \n",
    "            new_vals = np.max(np.array([opt1, opt2]), axis=0)\n",
    "\n",
    "            # print(new_vals)\n",
    "            # print(count)\n",
    "\n",
    "            if np.linalg.norm(new_vals - self.estimate_values) < 1e-10:\n",
    "                \n",
    "                self.policy = np.argmin(np.array([opt1, opt2]), axis=0)\n",
    "                break\n",
    "            self.estimate_values = new_vals\n",
    "        # print(count)\n",
    "    \n",
    "    def print_policy(self):\n",
    "        print(np.reshape(self.policy, newshape=(N_CHANNEL_STATES, TAU_LIMIT + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP Testing\n",
    "p_MDP = MDP()\n",
    "print(\"Pre training evaluation:\", env.evaluate_policy(p_MDP.policy))\n",
    "p_MDP.calculate_transitions(env)\n",
    "\n",
    "p_MDP.value_iter()\n",
    "np.set_printoptions(precision=2)\n",
    "print(p_MDP.estimate_transitions[0, :10, 10:20])\n",
    "p_MDP.print_policy()\n",
    "print(\"Post training evaluation:\", env.evaluate_policy(p_MDP.policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.set_printoptions(precision=2)\n",
    "print(p_MDP.count_of_transitions[1, 5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random\n",
    "p_random = Random_Policy()\n",
    "print(\"Random policy:\", env.evaluate_policy(p_random.policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy 1\n",
    "p_g_1 = Greedy_1()\n",
    "print(\"Pre training evaluation:\", env.evaluate_policy(p_g_1.policy))\n",
    "for ep in range(200):\n",
    "    env.reset()\n",
    "    for i in range(500):\n",
    "        p_g_1.train(env)\n",
    "p_g_1.print_policy()\n",
    "print(\"Post-training evaluated greedy policy:\", env.evaluate_policy(p_g_1.policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy 2\n",
    "p_g_2 = Greedy_2()\n",
    "print(\"Pre training evaluation:\", env.evaluate_policy(p_g_2.policy))\n",
    "for ep in range(200):\n",
    "    env.reset()\n",
    "    for i in range(500):\n",
    "        p_g_2.train(env)\n",
    "p_g_2.print_policy()\n",
    "print(\"Post-training evaluated greedy policy:\", env.evaluate_policy(p_g_2.policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
