{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CHANNEL_STATES = 2\n",
    "TAU_LIMIT = 9\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "TRANSMITED_POWER = 10\n",
    "WEIGHING_FACTOR = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self) -> None:\n",
    "        self.time = 1\n",
    "        self.A = np.array([[  1,  0, .5,  0],\n",
    "                             [  0,  1,  0, .5],\n",
    "                             [  0,  0,  1,  0],\n",
    "                             [  0,  0,  0,  1]])\n",
    "        self.Q = np.eye(4) * 2\n",
    "        self.C = np.array([[ 1.2,   0,   0,   0],\n",
    "                           [   0, 1.2,   0,   0]])\n",
    "        self.R = np.eye(2) * 0.6\n",
    "        self.sigma_2 = 8 # Noise power\n",
    "        self.packet_length = 10 \n",
    "        self.P_bar = 0.6613\n",
    "        self.zeta = TRANSMITED_POWER # Transmitted energy\n",
    "\n",
    "        self.state = random.randint(0, N_CHANNEL_STATES*(TAU_LIMIT + 1))\n",
    "\n",
    "        self.channel_gains = np.array([1, 1.5])\n",
    "        self.eta = np.array([[ .6, .4],\n",
    "                             [.45,.55]])\n",
    "        self.h_tau_P = []\n",
    "        self.h_tau_P.append(np.array([self.P_bar]))\n",
    "        self.h_tau_P.append(self.A @ self.A.T * self.P_bar)\n",
    "        for i in range(9):\n",
    "            self.h_tau_P.append(self.A @ self.h_tau_P[-1] @ self.A.T)\n",
    "        # print(self.h_tau_P)\n",
    "        # print(self.channel_gains)\n",
    "\n",
    "        self.p_js = np.zeros(N_CHANNEL_STATES)\n",
    "        for j in range(N_CHANNEL_STATES):\n",
    "            SNR = self.channel_gains[j] * self.zeta / self.sigma_2\n",
    "            self.p_js[j] = self.f(SNR)\n",
    " \n",
    "    def robot_step(self, action):\n",
    "        tau = self.state // N_CHANNEL_STATES\n",
    "        h = self.state % N_CHANNEL_STATES\n",
    "\n",
    "        self.time += 1\n",
    "        self.next_state(action)\n",
    "\n",
    "        return (np.trace(self.h_tau_P[tau]) if tau > 0 else self.h_tau_P[0]) + action * WEIGHING_FACTOR * self.zeta\n",
    "\n",
    "    def f(self, SNR):   # Calculates the probability of successful transmission based on SNR\n",
    "        sqrt_SNR = np.sqrt(SNR * 2)\n",
    "        I = scipy.special.erf(sqrt_SNR)\n",
    "        \n",
    "        return I**self.packet_length\n",
    "\n",
    "    def next_state(self, action):\n",
    "        tau = self.state // N_CHANNEL_STATES\n",
    "        h = self.state % N_CHANNEL_STATES\n",
    "\n",
    "        new_h = random.choices(population=range(N_CHANNEL_STATES), weights=self.eta[h, :], k=1)[0]\n",
    "        new_tau = tau + 1\n",
    "        if action == 1:\n",
    "            r = random.uniform(0,1)\n",
    "            if r < self.p_js[h]:\n",
    "                new_tau = 0\n",
    "        \n",
    "        if new_tau > TAU_LIMIT: # Ceiling functionality\n",
    "            new_tau = TAU_LIMIT\n",
    "        \n",
    "        self.state = (new_tau * N_CHANNEL_STATES) + new_h\n",
    "        \n",
    "    def reset(self):\n",
    "        self.time = 1\n",
    "        self.state = random.randint(0, N_CHANNEL_STATES - 1)\n",
    "\n",
    "    def reset_to_explore_starts(self):\n",
    "        self.time = 1\n",
    "        self.state = random.randint(0, N_CHANNEL_STATES * (TAU_LIMIT + 1) - 1)\n",
    "\n",
    "    def evaluate_policy(self, policy):\n",
    "        runs = 100\n",
    "        length_of_episode = 200\n",
    "        discount = 1\n",
    "        reward = 0\n",
    "        for run in range(runs):\n",
    "            self.reset()\n",
    "            discount = 1\n",
    "            for t in range(length_of_episode):\n",
    "                reward += self.robot_step(policy[self.state]) * discount\n",
    "                discount *= DISCOUNT_FACTOR\n",
    "                # print(reward, discount)\n",
    "        return reward / (runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self) -> None:\n",
    "        self.policy = np.array(np.random.rand(N_CHANNEL_STATES * (TAU_LIMIT + 1)) > 0.5, dtype=np.int32)\n",
    "            # This is initially a random policy, which will be finalized after values are calculated\n",
    "        self.estimated_costs = np.ones(shape=(N_CHANNEL_STATES * (TAU_LIMIT + 1), 2)) * 1000 \n",
    "            # This is to understand the estimated cost of taking an action in a given state, \n",
    "            # which is initialized to a very high value to prevent usage of unexplored states\n",
    "        self.count_of_transitions = np.zeros(shape=(2, N_CHANNEL_STATES * (TAU_LIMIT + 1), N_CHANNEL_STATES * (TAU_LIMIT + 1)))\n",
    "            # This is the count of transitions from state s to state s' given action a\n",
    "        self.estimate_transitions = np.zeros(shape=(2, N_CHANNEL_STATES * (TAU_LIMIT + 1), N_CHANNEL_STATES * (TAU_LIMIT + 1)))\n",
    "            # This is the normalized probability of the transition s -> s' given action a\n",
    "        self.estimate_values = np.zeros(N_CHANNEL_STATES * (TAU_LIMIT + 1))\n",
    "            # Once we calculate the transitions, this is our current estimate of the value of each state\n",
    "        self.n_states = N_CHANNEL_STATES * (TAU_LIMIT + 1)\n",
    "        \n",
    "    def calculate_transitions(self, env):\n",
    "        env.reset_to_explore_starts()\n",
    "        for run in range(1000):\n",
    "            # Exploring starts with any state starting\n",
    "            env.reset_to_explore_starts()\n",
    "            # To generate episodes\n",
    "            for t in range(100):\n",
    "                # Interact with the environment\n",
    "                s = env.state\n",
    "                a = 1 if run > 500 else 0\n",
    "                r = env.robot_step(action=a)\n",
    "                s_dash = env.state\n",
    "\n",
    "                # If this is the first exploration of the state, update the estimated cost :\n",
    "                if self.count_of_transitions[a, s, s_dash] == 0:\n",
    "                    self.estimated_costs[s, a] = r\n",
    "\n",
    "                # Update counts:\n",
    "                self.count_of_transitions[a, s, s_dash] += 1\n",
    "        \n",
    "        # Normalize the counts to get transition probabilities:\n",
    "        for s in range(N_CHANNEL_STATES * (TAU_LIMIT + 1)):\n",
    "            for a in range(2):\n",
    "                if np.sum(self.count_of_transitions[a, s, :]) != 0:\n",
    "                    self.estimate_transitions[a, s, :] = self.count_of_transitions[a, s, :] / np.sum(self.count_of_transitions[a, s, :])\n",
    "        # print(self.estimate_transitions)\n",
    "        # print(self.estimated_costs)\n",
    "\n",
    "    \n",
    "    def value_iter(self):\n",
    "        count = 0\n",
    "        for i in range(10):\n",
    "            count += 1\n",
    "            opt1 = self.estimated_costs[:, 0] + DISCOUNT_FACTOR * np.reshape(self.estimate_transitions[0, :, :], newshape=(self.n_states, self.n_states)) @ self.estimate_values\n",
    "                # Estimated value of choosing to not transmit (action = 0)\n",
    "            opt2 = self.estimated_costs[:, 1] + DISCOUNT_FACTOR * np.reshape(self.estimate_transitions[1, :, :], newshape=(self.n_states, self.n_states)) @ self.estimate_values\n",
    "                # Estimated value of choosing to transmit (action = 1)\n",
    "            \n",
    "            self.policy = np.argmin(np.array([opt1, opt2]), axis=0)\n",
    "\n",
    "            new_vals = np.min(np.array([opt1, opt2]), axis=0)\n",
    "\n",
    "            # print(new_vals)\n",
    "\n",
    "            if np.linalg.norm(new_vals - self.estimate_values) < 1:\n",
    "                break\n",
    "            self.estimate_values = new_vals\n",
    "        # print(count)\n",
    "\n",
    "    \n",
    "    def calculate_transitions_2(self, env):\n",
    "        self.new_transitions = np.zeros(shape=(2, N_CHANNEL_STATES * (TAU_LIMIT + 1), N_CHANNEL_STATES * (TAU_LIMIT + 1)))\n",
    "        for s in range(self.n_states - N_CHANNEL_STATES):\n",
    "            tau = s // N_CHANNEL_STATES\n",
    "            h = s % N_CHANNEL_STATES\n",
    "\n",
    "            new_tau = (tau + 1)\n",
    "            for new_h in range(2):\n",
    "                new_state_0 = (new_tau * N_CHANNEL_STATES) + new_h\n",
    "                new_state_1 = (tau * N_CHANNEL_STATES) + new_h\n",
    "                if new_state_0 < self.n_states:\n",
    "                    self.new_transitions[0, s, new_state_0] = env.eta[h, new_h]\n",
    "                    self.new_transitions[1, s, new_state_0] = env.eta[h, new_h] * (1 - env.p_js[h])\n",
    "                self.new_transitions[1, s, new_state_1] = env.eta[h, new_h] * env.p_js[h]\n",
    "            c = np.trace(env.h_tau_P[tau]) if tau > 0 else env.h_tau_P[0]\n",
    "            self.estimated_costs[s, 0] = c\n",
    "            self.estimated_costs[s, 1] = c + env.zeta\n",
    "        \n",
    "        \n",
    "        for s in range(self.n_states - N_CHANNEL_STATES, self.n_states):\n",
    "            tau = s // N_CHANNEL_STATES\n",
    "            h = s % N_CHANNEL_STATES\n",
    "            for new_h in range(2):\n",
    "                new_state = (tau * N_CHANNEL_STATES) + new_h\n",
    "                self.new_transitions[1, s, new_state] = env.eta[h, new_h] * env.p_js[h]\n",
    "            c = np.trace(env.h_tau_P[tau])  if tau > 0 else env.h_tau_P[0]\n",
    "            self.estimated_costs[s, 1] = c + env.zeta\n",
    "        print(self.new_transitions)\n",
    "\n",
    "    def value_iter_2(self):\n",
    "        count = 0\n",
    "        for i in range(10000):\n",
    "            count += 1\n",
    "            opt1 = self.estimated_costs[:, 0] + DISCOUNT_FACTOR * np.reshape(self.new_transitions[0, :, :], newshape=(self.n_states, self.n_states)) @ self.estimate_values\n",
    "                # Estimated value of choosing to not transmit (action = 0)\n",
    "            opt2 = self.estimated_costs[:, 1] + DISCOUNT_FACTOR * np.reshape(self.new_transitions[1, :, :], newshape=(self.n_states, self.n_states)) @ self.estimate_values\n",
    "                # Estimated value of choosing to transmit (action = 1)\n",
    "            \n",
    "            self.policy = np.argmin(np.array([opt1, opt2]), axis=0)\n",
    "\n",
    "            new_vals = np.min(np.array([opt1, opt2]), axis=0)\n",
    "\n",
    "            if np.linalg.norm(new_vals - self.estimate_values) < 1:\n",
    "                break\n",
    "            self.estimate_values = new_vals\n",
    "\n",
    "    \n",
    "    def print_policy(self):\n",
    "        print(np.reshape(self.policy, newshape=(N_CHANNEL_STATES, TAU_LIMIT + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "p_MDP = MDP()\n",
    "p_MDP.calculate_transitions(env)\n",
    "p_MDP.value_iter()\n",
    "p_MDP.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
