{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CHANNEL_STATES = 10\n",
    "TAU_LIMIT = 9\n",
    "DISCOUNT_FACTOR = 0.6\n",
    "WEIGHING_FACTOR = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self) -> None:\n",
    "        self.time = 1\n",
    "        self.A = 1.2\n",
    "        self.Q = 1\n",
    "        self.C = 1\n",
    "        self.R = 1\n",
    "        self.sigma_2 = 8 # Noise power\n",
    "        self.packet_length = 10 \n",
    "        self.P_bar = 0.6613\n",
    "        self.zeta = 45 # Transmitted energy\n",
    "\n",
    "        self.state = random.randint(0, N_CHANNEL_STATES*(TAU_LIMIT + 1))\n",
    "\n",
    "        self.channel_gains = np.array(range(10)) * 0.2 + 0.2\n",
    "        self.eta = np.array([[.91,.09,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "                             [ .9, .1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "                             [ .8,.12,.08,  0,  0,  0,  0,  0,  0,  0],\n",
    "                             [ .7,.11,.11,.08,  0,  0,  0,  0,  0,  0],\n",
    "                             [ .6,.11, .1, .1,.09,  0,  0,  0,  0,  0],\n",
    "                             [ .5,.18, .1,.08,.07,.07,  0,  0,  0,  0],\n",
    "                             [ .4,.28, .1,.06,.05,.06,.05,  0,  0,  0],\n",
    "                             [ .3, .1, .1, .1,.16,.14,.09,.01,  0,  0],\n",
    "                             [ .2,.11, .1, .1, .1,.05,.09,.13,.12,  0],\n",
    "                             [ .1, .1, .1, .1, .1, .1, .1, .1, .1, .1]])\n",
    "        self.h_tau_P = np.zeros(10)\n",
    "        self.h_tau_P[0] = self.P_bar\n",
    "        for i in range(10):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            self.h_tau_P[i] = self.A ** 2 * self.h_tau_P[i-1]\n",
    "        print(self.h_tau_P)\n",
    "        print(self.channel_gains)\n",
    "\n",
    "        self.p_js = np.zeros(N_CHANNEL_STATES)\n",
    "        for j in range(N_CHANNEL_STATES):\n",
    "            SNR = self.channel_gains[j] * self.zeta / self.sigma_2\n",
    "            self.p_js[j] = self.f(SNR)\n",
    "\n",
    "        \n",
    "    def robot_step(self, action):\n",
    "        tau = self.state // N_CHANNEL_STATES\n",
    "        h = self.state % N_CHANNEL_STATES\n",
    "\n",
    "        self.time += 1\n",
    "        self.next_state(action)\n",
    "\n",
    "        return self.h_tau_P[tau] + action * WEIGHING_FACTOR * self.zeta\n",
    "\n",
    "    def f(self, SNR):   # Calculates the probability of successful transmission based on SNR\n",
    "        sqrt_SNR = np.sqrt(SNR * 2)\n",
    "        I = scipy.special.erf(sqrt_SNR)\n",
    "        \n",
    "        return I**self.packet_length\n",
    "\n",
    "    def next_state(self, action):\n",
    "        tau = self.state // N_CHANNEL_STATES\n",
    "        h = self.state % N_CHANNEL_STATES\n",
    "\n",
    "        new_h = random.choices(population=range(N_CHANNEL_STATES), weights=self.eta[h, :], k=1)[0]\n",
    "        new_tau = tau + 1\n",
    "        if action == 1:\n",
    "            r = random.uniform(0,1)\n",
    "            SNR = self.channel_gains[h] * self.zeta / self.sigma_2\n",
    "            prob_packet_transmission = self.f(SNR)\n",
    "            if r < prob_packet_transmission:\n",
    "                new_tau = 0\n",
    "        \n",
    "        if new_tau > TAU_LIMIT: # Ceiling functionality\n",
    "            new_tau = TAU_LIMIT\n",
    "        \n",
    "        self.state = (new_tau * N_CHANNEL_STATES) + new_h\n",
    "        \n",
    "    def reset(self):\n",
    "        self.time = 1\n",
    "        self.state = random.randint(0, N_CHANNEL_STATES - 1)\n",
    "\n",
    "    def reset_to_explore_starts(self):\n",
    "        self.time = 1\n",
    "        self.state = random.randint(0, N_CHANNEL_STATES * (TAU_LIMIT + 1) - 1)\n",
    "\n",
    "    def evaluate_policy(self, policy):\n",
    "        runs = 100\n",
    "        length_of_episode = 200\n",
    "        discount = 1\n",
    "        reward = 0\n",
    "        for run in range(runs):\n",
    "            self.reset()\n",
    "            discount = 1\n",
    "            for t in range(length_of_episode):\n",
    "                reward += self.robot_step(policy[self.state]) * discount\n",
    "                discount *= DISCOUNT_FACTOR\n",
    "                # print(reward, discount)\n",
    "        return reward / (runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.66  0.95  1.37  1.97  2.84  4.09  5.9   8.49 12.23 17.61]\n",
      "[0.2 0.4 0.6 0.8 1.  1.2 1.4 1.6 1.8 2. ]\n"
     ]
    }
   ],
   "source": [
    "env = Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self) -> None:\n",
    "        self.policy = np.array(np.random.rand(N_CHANNEL_STATES * (TAU_LIMIT + 1)) > 0.5, dtype=np.int32)\n",
    "            # This is initially a random policy, which will be finalized after values are calculated\n",
    "        self.estimated_costs = np.ones(shape=(N_CHANNEL_STATES * (TAU_LIMIT + 1), 2)) * 1000 \n",
    "            # This is to understand the estimated cost of taking an action in a given state, \n",
    "            # which is initialized to a very high value to prevent usage of unexplored states\n",
    "        self.count_of_transitions = np.zeros(shape=(2, N_CHANNEL_STATES * (TAU_LIMIT + 1), N_CHANNEL_STATES * (TAU_LIMIT + 1)))\n",
    "            # This is the count of transitions from state s to state s' given action a\n",
    "        self.estimate_transitions = np.zeros(shape=(2, N_CHANNEL_STATES * (TAU_LIMIT + 1), N_CHANNEL_STATES * (TAU_LIMIT + 1)))\n",
    "            # This is the normalized probability of the transition s -> s' given action a\n",
    "        self.estimate_values = np.zeros(N_CHANNEL_STATES * (TAU_LIMIT + 1))\n",
    "            # Once we calculate the transitions, this is our current estimate of the value of each state\n",
    "        self.n_states = N_CHANNEL_STATES * (TAU_LIMIT + 1)\n",
    "    def calculate_transitions(self, env):\n",
    "        env.reset_to_explore_starts()\n",
    "        for run in range(1000):\n",
    "            # Exploring starts with any state starting\n",
    "            env.reset_to_explore_starts()\n",
    "            # To generate episodes\n",
    "            for t in range(1000):\n",
    "                # Interact with the environment\n",
    "                s = env.state\n",
    "                a = random.randint(0,1)\n",
    "                r = env.robot_step(action=a)\n",
    "                s_dash = env.state\n",
    "\n",
    "                # If this is the first exploration of the state, update the estimated cost :\n",
    "                if self.count_of_transitions[a, s, s_dash] == 0:\n",
    "                    self.estimated_costs[s, a] = r\n",
    "\n",
    "                # Update counts:\n",
    "                self.count_of_transitions[a, s, s_dash] += 1\n",
    "        \n",
    "        # Normalize the counts to get transition probabilities:\n",
    "        for s in range(N_CHANNEL_STATES * (TAU_LIMIT + 1)):\n",
    "            for a in range(2):\n",
    "                if np.sum(self.count_of_transitions[a, s, :]) != 0:\n",
    "                    self.estimate_transitions[a, s, :] = self.count_of_transitions[a, s, :] / np.sum(self.count_of_transitions[a, s, :])\n",
    "        print(self.estimate_transitions.shape)\n",
    "\n",
    "    def calculate_transitions_2(self, env):\n",
    "        self.new_transitions = np.zeros(shape=(2, N_CHANNEL_STATES * (TAU_LIMIT + 1), N_CHANNEL_STATES * (TAU_LIMIT + 1)))\n",
    "        for s in range(self.n_states - N_CHANNEL_STATES):\n",
    "            tau = s // N_CHANNEL_STATES\n",
    "            h = s % N_CHANNEL_STATES\n",
    "\n",
    "            new_tau = (tau + 1)\n",
    "            for new_h in range(10):\n",
    "                new_state_0 = (new_tau * N_CHANNEL_STATES) + new_h\n",
    "                new_state_1 = (tau * N_CHANNEL_STATES) + new_h\n",
    "                if new_state_0 < self.n_states:\n",
    "                    self.new_transitions[0, s, new_state_0] = env.eta[h, new_h]\n",
    "                    self.new_transitions[1, s, new_state_0] = env.eta[h, new_h] * (1 - env.p_js[h])\n",
    "                self.new_transitions[1, s, new_state_1] = env.eta[h, new_h] * env.p_js[h]\n",
    "            c = env.h_tau_P[tau]\n",
    "            self.estimated_costs[s, 0] = c\n",
    "            self.estimated_costs[s, 1] = c + env.zeta\n",
    "        \n",
    "        \n",
    "        for s in range(self.n_states - N_CHANNEL_STATES, self.n_states):\n",
    "            tau = s // N_CHANNEL_STATES\n",
    "            h = s % N_CHANNEL_STATES\n",
    "            for new_h in range(10):\n",
    "                new_state = (tau * N_CHANNEL_STATES) + new_h\n",
    "                self.new_transitions[1, s, new_state] = env.eta[h, new_h] * env.p_js[h]\n",
    "            c = env.h_tau_P[tau]\n",
    "            self.estimated_costs[s, 1] = c + env.zeta\n",
    "\n",
    "\n",
    "    def value_iter_2(self):\n",
    "        count = 0\n",
    "        for i in range(100):\n",
    "            count += 1\n",
    "            opt1 = self.estimated_costs[:, 0] + DISCOUNT_FACTOR * np.reshape(self.new_transitions[0, :, :], newshape=(self.n_states, self.n_states)) @ self.estimate_values\n",
    "                # Estimated value of choosing to not transmit (action = 0)\n",
    "            opt2 = self.estimated_costs[:, 1] + DISCOUNT_FACTOR * np.reshape(self.new_transitions[1, :, :], newshape=(self.n_states, self.n_states)) @ self.estimate_values\n",
    "                # Estimated value of choosing to transmit (action = 1)\n",
    "            \n",
    "            self.policy = np.argmin(np.array([opt1, opt2]), axis=0)\n",
    "\n",
    "            new_vals = np.max(np.array([opt1, opt2]), axis=0)\n",
    "\n",
    "            if np.linalg.norm(new_vals - self.estimate_values) < 1:\n",
    "                break\n",
    "            self.estimate_values = new_vals\n",
    "\n",
    "    def value_iter(self):\n",
    "        count = 0\n",
    "        for i in range(100):\n",
    "            count += 1\n",
    "            opt1 = self.estimated_costs[:, 0] + DISCOUNT_FACTOR * np.reshape(self.estimate_transitions[0, :, :], newshape=(self.n_states, self.n_states)) @ self.estimate_values\n",
    "                # Estimated value of choosing to not transmit (action = 0)\n",
    "            opt2 = self.estimated_costs[:, 1] + DISCOUNT_FACTOR * np.reshape(self.estimate_transitions[1, :, :], newshape=(self.n_states, self.n_states)) @ self.estimate_values\n",
    "                # Estimated value of choosing to transmit (action = 1)\n",
    "            \n",
    "            self.policy = np.argmin(np.array([opt1, opt2]), axis=0)\n",
    "\n",
    "            new_vals = np.max(np.array([opt1, opt2]), axis=0)\n",
    "\n",
    "            if np.linalg.norm(new_vals - self.estimate_values) < 1:\n",
    "                break\n",
    "            self.estimate_values = new_vals\n",
    "        # print(count)\n",
    "    def print_policy(self):\n",
    "        print(np.reshape(self.policy, newshape=(N_CHANNEL_STATES, TAU_LIMIT + 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_MDP = MDP()\n",
    "\n",
    "np.set_printoptions(precision=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_MDP.calculate_transitions_2(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "p_MDP.value_iter_2()\n",
    "# print(p_MDP.new_transitions[0, 10:20, 20:30])\n",
    "p_MDP.print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.8501629994905215"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.evaluate_policy(p_MDP.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
